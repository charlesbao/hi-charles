<!DOCTYPE html>
<html lang="en-us">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    


    
        <meta name="twitter:card" content="summary"/>
    



<meta name="twitter:title" content="HTTP代理扫描工具"/>
<meta name="twitter:description" content="
用Python实现的http代理扫描工具，利用爬虫抓取代理网站，并存储到数据库永久储存。"/>



  	<meta property="og:title" content=" HTTP代理扫描工具 &middot;  Hi!Charles" />
  	<meta property="og:site_name" content="Hi!Charles" />
  	<meta property="og:url" content="/post/http_scan/" />

    
  	<meta property="og:type" content="article" />

    <meta property="og:article:published_time" content="2017-06-02T00:00:00Z" />

    
    <meta property="og:article:tag" content="作品" />
    
    <meta property="og:article:tag" content="Python" />
    
    <meta property="og:article:tag" content="脚本" />
    
    

    <title>
       HTTP代理扫描工具 &middot;  Hi!Charles
    </title>

    <meta name="description" content="Chaos. Design. and Programming." />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="/images/favicon.ico">
	  <link rel="apple-touch-icon" href="/images/apple-touch-icon.png" />

    <link rel="stylesheet" type="text/css" href="/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="/css/nav.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
      
          <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Hi!Charles" />
      
      
    
    <meta name="generator" content="Hugo 0.21" />

    <link rel="canonical" href="/post/http_scan/" />

    

    
</head>
<body class="nav-closed">

  <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
        
        
        
    </ul>
    
    
    <a class="subscribe-button icon-feed" href="/index.xml">Subscribe</a>
    
</div>
<span class="nav-cover"></span>


 <div class="site-wrapper">




<header class="main-header post-head no-cover">
  <nav class="main-nav clearfix">


  
      <a class="blog-logo" href="/"><img src="/images/logo.svg" alt="Home" /></a>
  
  
      <a class="menu-button icon-feed" href="/index.xml">&nbsp;&nbsp;Subscribe</a>
  
  </nav>
</header>



<main class="content" role="main">




  <article class="post post" style="border:none;">

    <header class="post-header">
        <h1 class="post-title">HTTP代理扫描工具</h1>
        <small></small>

        <section class="post-meta">
            <img class="tags-thumb" src="/images/tags.png" alt="Tags image" nopin="nopin" />
       
            <a href="/tags/%E4%BD%9C%E5%93%81/">作品</a>
       
            <a href="/tags/python/">PYTHON</a>
       
            <a href="/tags/%E8%84%9A%E6%9C%AC/">脚本</a>
       
        <time class="post-date" datetime="2017-06-02T00:00:00Z">
            2 Jun 2017
        </time>
        </section>
    </header>

    <section class="post-content">
      <blockquote>
<p>用Python实现的http代理扫描工具，利用爬虫抓取代理网站，并存储到数据库永久储存。</p>
</blockquote>

<p>数据库使用 sqlite3<br />
依赖<br />
pip install requesocks bs4</p>

<pre><code># encoding=utf8
from multiprocessing.dummy import Pool as ThreadPool
from bs4 import BeautifulSoup

import requesocks as requests
import datetime
import json
import time
import sqlite3
import re
import os

def recordProxy(ip, port, country, valid = 0, req_speed = None, proxies = None):

    checkDate = time.strftime('%Y-%m-%d %H:%M:%S')
    executeScript = &quot;SELECT IP from PROXY WHERE IP = '%s'&quot; \
                    % ip
    cursor = conn.executescript(executeScript)
    if valid == 0:
        # 检查是否有重复
        for _ in cursor:
            executeScript = &quot;UPDATE PROXY set VALID = 0, CHECK_DATE = '%s' where IP='%s'&quot; \
                            % (checkDate,ip)
            conn.executescript(executeScript)
            return False
        # 插入一条无效连接代理IP
        executeScript = &quot;INSERT INTO PROXY (IP,PORT,COUNTRY,CHECK_DATE,VALID) VALUES ('%s', %d, '%s', '%s', 0)&quot; \
                        % (ip, port, country,checkDate)
        conn.executescript(executeScript)
    else:
        # 检查是否有重复
        for _ in cursor:
            executeScript = &quot;UPDATE PROXY set REQ_SPEED='%f', PROXIES='%s', VALID = 1, CHECK_DATE = '%s' where IP='%s'&quot; \
                            % (req_speed,proxies,checkDate, ip)
            conn.executescript(executeScript)
            return True
        executeScript = &quot;INSERT INTO PROXY (IP,PORT,COUNTRY,REQ_SPEED,PROXIES,CHECK_DATE,VALID) VALUES ('%s', %d, '%s', '%f', '%s', '%s', 1)&quot;  \
                        % (ip, port, country,req_speed,proxies,checkDate)
        conn.executescript(executeScript)

def checkProxy(ProxyValue):

    (ip,port,country) = ProxyValue

    for proxySuffix in ['http','https']:
        try:
            startTime = datetime.datetime.now()
            session.proxies = {'http': '%s://%s:%d' % (proxySuffix,ip,port)}
            resHeaders = session.get('http://www.baidu.com/img/baidu_jgylogo3.gif', timeout=3).headers
        except:
            continue
        else:
            if resHeaders['content-length'] == '705':
                endTime = datetime.datetime.now()
                req_speed = round(float((endTime - startTime).microseconds) / 1000000, 4)
                # 插入一条有效连接代理IP
                print ip, proxySuffix, req_speed
                recordProxy(ip, port, country, valid=1,req_speed=req_speed,proxies=proxySuffix)
                return True
    print ip,'error'
    # 插入一条无效连接代理IP
    recordProxy(ip,port,country,valid=0)
    return False

def scanProxy(ProxyArr):
    pool = ThreadPool(5)
    pool.map(checkProxy, ProxyArr)
    pool.close()
    pool.join()

def getCountry(ip):

    try:
        # 获取ip所在地
        contents = requests.get('http://int.dpool.sina.com.cn/iplookup/iplookup.php?format=json&amp;ip=%s' % ip).content
    except:
        print 'network:country api error'
        os._exit(1)
    else:
        try:
            jsonObj = json.loads(contents)
        except:
            print 'jsonLoad:country api error'
            os._exit(1)
        else:
            if 'country' in contents:
                return jsonObj['country']
            else:
                print 'hasNoCountryID:country api error'
                os._exit(1)

def getProxy():

    for page in range(1,250):
        try:
            # 需要扫描的网址
            contents = requests.get('http://www.xicidaili.com/wn/%d' % page,headers = header, timeout = 5).content
        except:
            print 'proxyError'
        else:
            soup = BeautifulSoup(contents, &quot;html.parser&quot;)
            ProxyArr = []
            if len(soup.select('tr')) == 1:
                print 'page',page,'end'
                os._exit(0)
            for tr in soup.select('tr'):
                td = tr.select('td')
                if len(td) &gt; 3:
                    ip = td[1].text
                    if not re.search('.+\..+\..+\..+',ip,re.S) is None:
                        try:
                            port = int(td[2].text)
                        except:
                            print td[2].text
                            os._exit(1)
                        else:
                            country = getCountry(ip)
                            ProxyArr.append((ip,port,country))

            print 'begin scan', len(ProxyArr)
            #begin to check proxy validity
            scanProxy(ProxyArr)

header = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36'
}
session = requests.session()
session.headers.update(header)

# 数据库设计
# ID    IP      PORT    COUNTRY     PROXIES     REQ_SPEED   CHECK_DATE  VALID
# int   text    int     text        text        float       text        int
#       32      8       32          8           8           64          2

# 数据库地址
conn = sqlite3.connect('/Users/chalresbao/Documents/proxy.db',check_same_thread=False)
getProxy()
conn.close()
</code></pre>
    </section>


  <footer class="post-footer">

    





<section class="author">
  <h4><a href="/">Charles Bao</a></h4>
  
  <p>Chaos. Design. and Programming. This is me.</p>
  
  <div class="author-meta">
    <span class="author-location icon-location">Jiangsu, China</span>
    <span class="author-link icon-link"><a href="https://charlesbao.com">https://charlesbao.com</a></span>
  </div>
</section>



    




  </footer>
</article>

</main>
<aside class="read-next">
  
      <a class="read-next-story prev" style="no-cover" href="/post/alioss-python/">
          <section class="post">
              <h2>Python 阿里云oss存储 小文件上传下载</h2>
          </section>
      </a>
  
</aside>


    <footer class="site-footer clearfix">
        <section class="copyright"><a href="www.miitbeian.gov.cn">苏ICP备16024813号</a> | <a href="/">Hi!Charles 个人的版权所有</a></section>
    </footer>
    </div>
    <script type="text/javascript" src="/js/jquery.js"></script>
    <script type="text/javascript" src="/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="/js/index.js"></script>
    
</body>
</html>

